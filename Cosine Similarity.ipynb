{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%run C:/Users/siebe/Documents/JT_Charts.ipynb\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stopwords_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For reference: \n",
    "### a high cosine similarity means text is similar\n",
    "### a low cosine similarity means text is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same 1.0\n",
      "opposite 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "def cosine(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    cosine_similarity = dot_product / (norm_a * norm_b)\n",
    "    return cosine_similarity\n",
    "\n",
    "a = [1,1,1,1,1,1,0,0,0,0]\n",
    "b = [0,0,0,0,0,0,1,1,1,1]\n",
    "print(\"same\", '{:1.1f}'.format(cosine(a, a)))\n",
    "print(\"opposite\", '{:1.1f}'.format(cosine(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[Financial Analysis and Commentary]\\nPolitical...</td>\n",
       "      <td>US newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>China appears to be making its long-expected m...</td>\n",
       "      <td>US newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[Financial Analysis and Commentary]\\nIn his pu...</td>\n",
       "      <td>US newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The possibility that Beijing would call in its...</td>\n",
       "      <td>US newspapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Hong Kong -- President Xi Jinping wanted Tuesd...</td>\n",
       "      <td>US newspapers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      newspaper\n",
       "0  [Financial Analysis and Commentary]\\nPolitical...  US newspapers\n",
       "1  China appears to be making its long-expected m...  US newspapers\n",
       "2  [Financial Analysis and Commentary]\\nIn his pu...  US newspapers\n",
       "3  The possibility that Beijing would call in its...  US newspapers\n",
       "4  Hong Kong -- President Xi Jinping wanted Tuesd...  US newspapers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corpus\n",
    "## Chinese newspapers\n",
    "china_daily=pd.read_csv('China Daily.csv')\n",
    "people_daily=pd.read_csv('People\\'s Daily.csv')\n",
    "xinhua_agent=pd.read_csv('Xinhua Agent.csv')\n",
    "\n",
    "CH=pd.concat([china_daily, people_daily, xinhua_agent],axis=0,ignore_index=True)\n",
    "\n",
    "## US newspapers\n",
    "Wall_Street_Journal = pd.read_csv('Wall Street Journal.csv')\n",
    "Washington_post = pd.read_csv('Washington Post.csv')\n",
    "New_York_Times = pd.read_csv('New York Times.csv')\n",
    "\n",
    "US=pd.concat([Wall_Street_Journal, New_York_Times, Washington_post],axis=0,ignore_index=True)\n",
    "\n",
    "# Adjust columns of dataset and drop missings\n",
    "def corpus_create(news, val):\n",
    "    news = news.drop(['title'], axis=1)\n",
    "    news = news.dropna(how='all')\n",
    "    news['newspaper'] = val\n",
    "    return news\n",
    "\n",
    "CH = corpus_create(CH, 'Chinese newspapers')\n",
    "US = corpus_create(US, 'US newspapers')\n",
    "\n",
    "# Combine US and Chinese newspapers\n",
    "corpus = pd.concat([US, CH],axis=0,ignore_index=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"\\'s\", \"china daily\", \"people daily\", \"xinhua\", \n",
    "            \"wall street journal\", \"new york times\", \"washington post\",\n",
    "            \"nyt\", \"washington\",\n",
    "            \"editorial\", \"commentary\", \"crdito\", \"keith\"]\n",
    "\n",
    "# Case removal, non-alpha removal, keyword removal, and stemming\n",
    "def corpus_clean(corpus):\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [re.sub(r'[^a-zA-Z]', \"\",y.lower()) for y in x])\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [y for y in x if y not in keywords])\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "    corpus['text'] = pd.Series(corpus['text']).astype(str)\n",
    "    return corpus['text']\n",
    "\n",
    "corpus = corpus_clean(corpus)\n",
    "CH = corpus_clean(CH)\n",
    "US = corpus_clean(US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of terms 17054\n"
     ]
    }
   ],
   "source": [
    "# DTM\n",
    "vectorizer = CountVectorizer(binary=True) \n",
    "vectorizer.fit(corpus)\n",
    "CH_dtm = vectorizer.transform(CH)\n",
    "US_dtm = vectorizer.transform(US)\n",
    "\n",
    "print(\"Count of terms\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US and Chinese newspaper cosine similarity\n",
      "0.5508\n"
     ]
    }
   ],
   "source": [
    "# Collapse all terms\n",
    "a = [max(x) for x in zip(*CH_dtm.toarray())]\n",
    "b = [max(x) for x in zip(*US_dtm.toarray())]\n",
    "\n",
    "cosine_similarity = cosine(a, b)\n",
    "\n",
    "print(\"US and Chinese newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US and Chinese newspaper cosine similarity (0.55)\n",
    "### Cosine similarity is mid-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese newspapers\n",
    "china_daily = corpus_create(china_daily, 'China Daily')\n",
    "people_daily = corpus_create(people_daily, 'People\\'s Daily')\n",
    "xinhua_agent = corpus_create(xinhua_agent, 'Xinhua Agent')\n",
    "\n",
    "corpus=pd.concat([china_daily, people_daily, xinhua_agent],axis=0,ignore_index=True)\n",
    "\n",
    "corpus = corpus_clean(corpus)\n",
    "china_daily = corpus_clean(china_daily)\n",
    "people_daily = corpus_clean(people_daily)\n",
    "xinhua_agent = corpus_clean(xinhua_agent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of terms 8333\n"
     ]
    }
   ],
   "source": [
    "# DTM\n",
    "vectorizer = CountVectorizer(binary=True) \n",
    "vectorizer.fit(corpus)\n",
    "china_daily_dtm = vectorizer.transform(china_daily)\n",
    "people_daily_dtm = vectorizer.transform(people_daily)\n",
    "xinhua_agent_dtm = vectorizer.transform(xinhua_agent)\n",
    "\n",
    "print(\"Count of terms\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China Daily and People Daily newspaper cosine similarity\n",
      "0.6789\n",
      "\n",
      "People Daily and Xinhua newspaper cosine similarity\n",
      "0.7153\n",
      "\n",
      "China Daily and Xinhua newspaper cosine similarity\n",
      "0.6364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collapse all terms\n",
    "a = [max(x) for x in zip(*china_daily_dtm.toarray())]\n",
    "b = [max(x) for x in zip(*people_daily_dtm.toarray())]\n",
    "c = [max(x) for x in zip(*xinhua_agent_dtm.toarray())]\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(a, b)\n",
    "\n",
    "print(\"China Daily and People Daily newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(b, c)\n",
    "\n",
    "print(\"People Daily and Xinhua newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(a, c)\n",
    "\n",
    "print(\"China Daily and Xinhua newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese newspapers cosine similarity (0.64-0.72)\n",
    "### Cosine similarity is higher meaning they are more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US newspapers\n",
    "Wall_Street_Journal = corpus_create(Wall_Street_Journal, 'Wall Street Journal')\n",
    "Washington_post = corpus_create(Washington_post, 'Washington Post')\n",
    "New_York_Times = corpus_create(New_York_Times, 'New York Times')\n",
    "\n",
    "corpus=pd.concat([Wall_Street_Journal, Washington_post, New_York_Times],axis=0,ignore_index=True)\n",
    "\n",
    "corpus = corpus_clean(corpus)\n",
    "Wall_Street_Journal = corpus_clean(Wall_Street_Journal)\n",
    "Washington_post = corpus_clean(Washington_post)\n",
    "New_York_Times = corpus_clean(New_York_Times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of terms 14848\n"
     ]
    }
   ],
   "source": [
    "# DTM\n",
    "vectorizer = CountVectorizer(binary=True) \n",
    "vectorizer.fit(corpus)\n",
    "Wall_Street_Journal_dtm = vectorizer.transform(Wall_Street_Journal)\n",
    "Washington_post_dtm = vectorizer.transform(Washington_post)\n",
    "New_York_Times_dtm = vectorizer.transform(New_York_Times)\n",
    "\n",
    "print(\"Count of terms\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal and Washington Post newspaper cosine similarity\n",
      "0.6195\n",
      "\n",
      "Washington Post and New York Times newspaper cosine similarity\n",
      "0.6303\n",
      "\n",
      "Wall Street Journal and New York Times newspaper cosine similarity\n",
      "0.6220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collapse all terms\n",
    "a = [max(x) for x in zip(*Wall_Street_Journal_dtm.toarray())]\n",
    "b = [max(x) for x in zip(*Washington_post_dtm.toarray())]\n",
    "c = [max(x) for x in zip(*New_York_Times_dtm.toarray())]\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(a, b)\n",
    "\n",
    "print(\"Wall Street Journal and Washington Post newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(b, c)\n",
    "\n",
    "print(\"Washington Post and New York Times newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = cosine(a, c)\n",
    "\n",
    "print(\"Wall Street Journal and New York Times newspaper cosine similarity\")\n",
    "print('{:1.4f}'.format(cosine_similarity))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US newspapers cosine similarity (0.62-0.63)\n",
    "### Cosine similarity is a little high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
